Below is a draft template for forum community owners and operations to handle the pending UK [Online Safety Act (OSA)](https://www.legislation.gov.uk/ukpga/2023/50/contents/enacted) requirements outlined at https://www.gov.uk/government/publications/online-safety-act-explainer/online-safety-act-explainer and [four step risk assessment process](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/quick-guide-to-online-safety-risk-assessments/) (see [summary here](/risk-assessment-overview.md)).

Consult the [Online Safety Regulation Checker](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/check/) to see if it applies to you. And check [important dates for Online Safety Act](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/important-dates-for-online-safety-compliance/).

----

1. Does your online service have links with the UK?

Your online service has links with the UK if:

* UK users are a target market for your service; or
* It has a significant number of UK users

UK users could include anyone based in the UK who visits or interacts with your website, app, or other kind of online service. You may refer to them in a variety of ways such as your customers, clients, subscribers, visitors or similar terms.

UK users can be individual people or entities (e.g. organisations) in the UK. They include both those who register with you (e.g. create an account) and those who don’t.

2. Do you provide a “user-to-user” service?

The regulations apply to “user-to-user” services.

A user-to-user service is an online service that allows its users to interact with each other. This includes the ability to generate, upload or share content, such as images, videos, messages or comments, with other users of that online service.

User-to-user services include online services which allow private messaging between users.

Here, you do not need to consider content that you (the online service provider) publish directly, as it is not relevant for this question.

3. Do you provide a search service?

A search service is an online service which is, or includes, a search engine. A search engine is a feature which enables users to search more than one website and/or database.

However, if your online service only searches one website or database, it will not be classed as a search service. For example, where a website has a search bar feature to find content internally on that website.

Some online services use or embed a search engine provided by a third party (e.g. as a “plug in”). Where this embedded search engine is not controlled by you but by a third party, the regulations related to search services are likely to apply to the third party.

4. Does your online service publish or display pornographic content?

The Act defines pornographic content as content that was produced solely or principally for the purpose of sexual arousal. This only includes audio, image and video content. It does not include text or written content.

Different parts of the Act will apply if the pornographic content included on a service is published or displayed by the provider of the service (as opposed to content generated by users).

5. Do any exemptions apply to the content on your online service?

Some types of user-to-user service are exempt from the Act. Generally, this is because there are limits to the ways users can communicate on your online service or there are limits to the type of content users can generate or share on your online service.

Your online service will be exempt if:

The only way users can communicate on your online service is by email, SMS, MMS and/or one-to-one live aural communications; and/or
Users can only interact with content generated by your business/the provider of the online service. Such interactions include: comments, likes/dislikes, ratings/reviews of your content including using emojis or symbols. For example, this exemption would cover online services where the only content users can upload or share is comments on media articles you have published, or reviews of goods and services your business provides. It would not apply if users can interact with content generated by other users.

6. Do any exemptions apply to your online service?

Some online services are exempt from the Act because they are internal business services or because they are provided by a public body, or by an education or childcare provider.

----

**Does The Online Safety Act Apply?**

Online Safety Act will apply if you replied with:

Does your online service have links with the UK?
* Your answer:  Yes

Do you provide a “user-to-user” service?
* Your answer:  Yes

Do you provide a search service?
* Your answer:  Yes

Does your online service publish or display pornographic content?
* Your answer:  No, we don’t publish/display any pornographic content

Do any exemptions apply to the content on your online service?
* Your answer:  No, my service is not limited to these types of content

Do any exemptions apply to your online service?
* Your answer:  No, none of the above applies

----

# Draft Template Table of Contents

1.  [Introduction](#1-introduction)
2.  [Scope](#2-scope)
    *   [Illegal Content](#illegal-content)
    *   [Harmful Content](#harmful-content)
    *   [Community Guidelines](#community-guidelines)
    *   [User Conduct](#user-conduct)
3.  [Principles](#3-principles)
    *   [User Safety](#user-safety)
    *   [Proportionality](#proportionality)
    *   [Transparency](#transparency)
    *   [Fairness](#fairness)
    *   [Efficiency](#efficiency)
    *   [Accountability](#accountability)
    *   [Good Faith Compliance](#good-faith-compliance)
    *   [Freedom of Expression](#freedom-of-expression)
    *   [Privacy](#privacy)
4.  [Our Duties Under the Online Safety Act (OSA)](#4-our-duties-under-the-online-safety-act-osa)
    *   [Risk Assessments](#risk-assessments)
    *   [Duties Regarding Illegal Content](#duties-regarding-illegal-content)
    *   [Duties Regarding Protection of Children (if applicable)](#duties-regarding-protection-of-children-if-applicable)
    *   [Duties Regarding Freedom of Expression and Privacy](#duties-regarding-freedom-of-expression-and-privacy)
5.  [Complaint Process](#5-complaint-process)
    *   [5.1 How to Submit a Complaint:](#51-how-to-submit-a-complaint)
    *   [5.2 Information to Include in Your Complaint:](#52-information-to-include-in-your-complaint)
    *   [5.3 Complaint Handling:](#53-complaint-handling)
6.  [Content Moderation and Safety Measures](#6-content-moderation-and-safety-measures)
7.  [Illegal Content Risk Assessment Process](#7-illegal-content-risk-assessment-process)
    *   [7.1 Step 1: Understanding the Kinds of Illegal Content to be Assessed](#71-step-1-understanding-the-kinds-of-illegal-content-to-be-assessed)
    *   [7.2 Step 2: Assessing the Risk of Harm](#72-step-2-assessing-the-risk-of-harm)
    *   [7.3 Step 3: Deciding on Measures, Implementing, and Recording](#73-step-3-deciding-on-measures-implementing-and-recording)
    *   [7.4 Step 4: Reporting, Reviewing, and Updating](#74-step-4-reporting-reviewing-and-updating)
    *   [7.5 Risk Assessment Findings](#75-risk-assessment-findings)
8. [Children's Access and Risk Assessment](#8-childrens-access-and-risk-assessment)
   * [8.1 Children's Access Assessment](#81-childrens-access-assessment)
   * [8.2 Children's Risk Assessment](#82-childrens-risk-assessment)
   * [8.3 Age Assurance Assessment](#83-age-assurance-assessment)
     * [8.3.1 Compliance Timeline](#831-compliance-timeline)
     * [8.3.2 Age Assurance Approach](#832-age-assurance-approach)
     * [8.3.3 Technical Implementation](#833-technical-implementation)
     * [8.3.4 Privacy and User Experience Considerations](#834-privacy-and-user-experience-considerations)
     * [8.3.5 Documentation and Review Process](#835-documentation-and-review-process)
9.  [Freedom of Expression, Privacy, and Encryption](#9-freedom-of-expression-privacy-and-encryption)
10. [Record Keeping](#10-record-keeping)
11. [Review and Updates](#11-review-and-updates)
12. [Contact Us](#12-contact-us)
13. [Limitations and Disclaimer](#13-limitations-and-disclaimer)
14. [User Redress Mechanisms](#14-user-redress-mechanisms)
    *   [14.1 Internal Redress for Content Moderation Decisions:](#141-internal-redress-for-content-moderation-decisions)
    *   [14.2 Complaints About Our Enforcement of Terms of Service:](#142-complaints-about-our-enforcement-of-terms-of-service)
    *   [14.3 External Redress:](#143-external-redress)
15. [Definitions and Interpretation](#15-definitions-and-interpretation)
16. [Index of Recommended Measures](#16-index-of-recommended-measures)

# [Forum Name] Online Safety Act Compliance and Complaint Procedure

## 1. Introduction

[Forum Name] is a community forum dedicated to [Forum's Purpose]. We are committed to fostering a safe and positive online environment for all our users. We fully acknowledge the UK Online Safety Act 2023 (OSA) and its aim to regulate online speech and media to make the UK the safest place in the world to be online.

As a small, volunteer-run forum, we face extreme challenges in meeting the full scope of the OSA's requirements. The Act imposes substantial legal and operational burdens that are disproportionately difficult for small platforms like ours to manage. We have no dedicated legal, technical, or content moderation staff. Our resources are severely limited, and we rely entirely on volunteers.

Nevertheless, we are committed to taking all reasonably practicable steps to comply with the Act within our extremely limited resources. This document outlines our approach to online safety, including our complaint process, the measures we are taking to address illegal and harmful content, our understanding of our duties under the Act, and our user redress mechanisms.

We also transparently highlight the severe limitations we face in fully implementing the Act's requirements and emphasize that our efforts represent a "best effort" attempt at compliance, given our circumstances. We have used Ofcom's published guidance, including the "Risk Assessment Guidance and Risk Profiles" (December 16, 2024), to inform our approach, and we will continue to monitor their publications for updates.

We have reviewed and incorporated, as far as practicable given our resources, the measures detailed in Ofcom's Illegal Content Codes of Practice for user-to-user services (May 2024) into our online safety approach. We understand that implementing these recommended measures, to the extent that we are able, will be considered as compliance with the relevant duties under the OSA. We have indicated throughout this document where these measures have been integrated, and where we have been forced to implement alternative measures due to resource constraints. We understand that our chosen alternative measures must still meet the requirements of the Act, and we must also maintain a record of what they are, how they work and how we consider them to meet the relevant duties, including how they comply with the duty to have particular regard to the importance of protecting freedom of expression and privacy.

We understand that implementing the recommended measures in the Codes will involve the processing of personal data. We are committed to taking a 'data protection by design and default' approach and will comply fully with data protection law, including guidance issued by the Information Commissioner's Office (ICO), when taking measures for the purpose of complying with our online safety duties under the Act.


## 2. Scope

This policy applies to all users of [Forum Name]. It covers our approach to:

* Illegal Content: Content that violates UK law, as defined by the OSA, including the "priority offences" listed in the Act and detailed in subsequent Ofcom guidance (including the 17 categories of priority illegal content and other illegal content).

* Harmful Content: Content that may not be illegal but could cause significant harm, especially to children, as outlined in the OSA and subsequent Ofcom guidance. We are particularly aware of the categories of "Primary Priority Content" and "Priority Content" harmful to children, as well as content related to self-harm, eating disorders, suicide, and various forms of abuse and hate.

* Community Guidelines: Our forum rules [Link to Rules].

* User Conduct: The behavior of users on our platform.

## 3. Principles

Our online safety approach is guided by the following principles:

* User Safety: We prioritize the safety and well-being of our users, particularly children.

* Proportionality: We will take actions that are proportionate to the risk of harm, considering the nature of our forum, our user base, and our extremely limited resources. We will focus our efforts on addressing the most serious risks and harms, particularly those involving illegal content and content prioritized by Ofcom.

* Transparency: We will be as transparent as possible about our online safety practices, the challenges we face, the limitations of our resources, and the difficult choices we have to make in attempting to comply with the OSA. We will clearly communicate our processes and limitations to our users.

* Fairness: We will strive to treat all users and complaints fairly and impartially, recognizing that our capacity for investigation and detailed review is severely limited.

* Efficiency: We will address safety issues as efficiently as our volunteer capacity allows. We must prioritize issues based on their severity, potential impact, and the likelihood of causing significant harm, as outlined by Ofcom's guidance.

* Accountability: We are accountable for our online safety efforts and will regularly review our practices, subject to volunteer availability. These reviews will be limited in scope but informed by Ofcom's guidance.

* Good Faith Compliance: We are committed to making a good faith effort to comply with the OSA within our significant limitations as a small, volunteer-run platform. We will document our efforts and decisions to demonstrate this commitment, using Ofcom's resources as a guide.

* Freedom of Expression: We recognize the importance of freedom of expression, as emphasized in the OSA. We will seek to balance safety with this fundamental right, particularly in the context of journalistic and democratically important content, as defined by the Act. However, our capacity to make nuanced judgments in this area is extremely limited.

* Privacy: We are committed to protecting user privacy, as outlined in the OSA, and will handle personal data in accordance with data protection laws. We are deeply concerned about the potential implications of the OSA for privacy, including the provisions related to encrypted services and the requirement to scan for certain types of content. We do not currently use end-to-end encryption on this forum.

* Good Faith Compliance: We are committed to making a good faith effort to comply with the OSA and to adopt, where practicable, the measures outlined in Ofcom's Codes of Practice within our significant limitations as a small, volunteer-run platform. We will document our efforts and decisions to demonstrate this commitment, using Ofcom's resources as a guide, particularly the index of recommended measures in Section 3 of the Codes.

## 4. Our Duties Under the Online Safety Act (OSA)

We understand that the OSA imposes several duties on online services, including "user-to-user services" like our forum. These duties include, but are not limited to:

* Risk Assessments:

    * Illegal Content Risk Assessment: Regularly assessing the risk of users encountering illegal content on our forum, giving particular consideration to our service's characteristics - such as its user base and functionalities and taking into account Ofcom's Risk Profiles. (See Section 7) We will prioritize "priority offences" as defined in the Act and follow Ofcom's guidance on conducting these assessments.

    * Children's Access Assessment: Assessing whether children are likely to access our forum. (See Section 8) We will use Ofcom's guidance to inform this assessment and document our decision-making process.

    * Children's Risk Assessment: If children are likely to access our forum, assessing the risks of harm to children from content on our service, paying particular attention to "Primary Priority Content" and "Priority Content" as defined in the Act and detailed in Ofcom's guidance. (See Section 8)

* Duties Regarding Illegal Content:

    * Taking proportionate steps to mitigate and manage the risks identified in our illegal content risk assessment. We will focus on the most serious and prevalent illegal content due to our resource constraints, and we will prioritize our actions based on the likelihood and severity of harm, using Ofcom's codes of practice to guide our choices.

    * Operating a complaints procedure that allows for reporting of illegal content.

    * Swiftly removing illegal content upon becoming aware of it, to the best of our ability given our limited resources and the need to prioritize the most serious cases, as guided by Ofcom's recommendations.

* Duties Regarding Protection of Children (if applicable):

    * Taking proportionate measures to prevent children from encountering harmful content, to the extent feasible given our limited resources and technology. We will prioritize "Primary Priority Content" and "Priority Content" as defined in the Act and Ofcom's guidance, and we will use Ofcom's codes of practice to inform our approach.

    * Including provisions in our terms of service to protect children from harm.

    * Implement highly effective age assurance measures using OFCom's to be provided Protection of Children Codes and children’s risk assessment guidance in April 2025 by [July 2025](https://www.ofcom.org.uk/online-safety/protecting-children/age-checks-to-protect-children-online/) if we determine through our children's access assessment that children are likely to access our service, or if our service contains pornographic content. These measures must:

        * Be technically accurate, robust, reliable, and fair in determining whether users are children
        * Protect user privacy while maintaining effectiveness
        * Be implemented before users can access any content requiring age verification
        * Be regularly reviewed and updated to maintain effectiveness

* Duties Regarding Freedom of Expression and Privacy:

    * Having particular regard to the importance of protecting users' rights to freedom of expression, as defined in the Act, when implementing safety measures. Our capacity to make nuanced judgments on "democratically important" or "journalistic" content is severely limited.

    * Considering user privacy when making decisions about content moderation and data handling. We are concerned about the potential implications of the OSA for privacy, including the provisions related to encrypted services and content scanning.

    * We will prioritize our compliance efforts, focusing on the implementation of measures outlined in Ofcom's Codes of Practice where they are most applicable to our service, considering our risk assessments and resource constraints.

* We understand that the duties outlined in the OSA do not extend to certain types of content or activities on combined services, as defined in section 8 of the Act. Specifically, we note that our duties do not extend to regulated provider pornographic content (if applicable), search content, or other content encountered as a result of subsequent interactions with internet services following a search request. We also note that our duties extend only to the design, operation and use of the service in the United Kingdom and the design, operation and use of the service as it affects United Kingdom users of the service.


## 5. Complaint Process

### 5.1 How to Submit a Complaint:

* "Report" Button: This is the primary and most efficient method for reporting content or users. Use the "Report" button located on each post, private message, or user profile. When reporting, you must: 

* Provide a detailed reason for your report.

* Specify the nature of the issue, clearly indicating whether you believe the content or behavior is illegal under the OSA, harmful (and if so, to whom), or a violation of our community guidelines.

* Reference specific OSA provisions, Ofcom guidance (including the Codes of Practice), or sections of our community guidelines where possible. This will assist our volunteer moderators in assessing the complaint. Use Ofcom's published resources to help identify the relevant provisions.

* Indicate the urgency of the matter if you believe it involves an imminent threat to life or serious illegal activity.

* Email: You may send an email to [Designated Complaints Email - if you have one. If not, explain the limitations and that this method should only be used for complex issues or when the report button is not applicable]. This method may have significantly slower response times due to our volunteer capacity. Use this method only when the "Report" button is not applicable or when you need to provide extensive documentation.

* Private Message: Contact a moderator directly via private message (see moderator list [Link]). This method may have very slow response times due to volunteer availability and should be reserved for urgent matters or when other methods are not suitable. We cannot guarantee a timely response via this method.

We will prioritize complaints that involve potential violations of the OSA, especially those related to priority offenses or serious harm, and align with the measures outlined in Ofcom's Codes of Practice. We will give appropriate priority to processing complaints received from trusted flaggers via our dedicated reporting channel, as detailed in section 5.4 of this document, where that reporting channel is in accordance with measure ICU D14.

### 5.2 Information to Include in Your Complaint:

When submitting a complaint, please provide as much of the following information as possible:

* Link: A direct link to the specific content (post, message, profile) you are reporting.

* Username: The username of the user who posted the content or exhibited the behavior.

* Description: A clear and detailed description of the content or behavior you are reporting. Explain why you believe it is illegal under the OSA, harmful (and to whom), or a violation of our community guidelines. Be specific about the type of illegal or harmful content you are reporting (e.g., CSAM, terrorist content, content promoting self-harm, hate speech). Reference specific sections of the OSA, Ofcom's guidance (including their codes of practice), or our community guidelines where applicable.

* Evidence: Provide screenshots or other supporting evidence, if possible and relevant.

* Your Details (Optional): Your username and email address (if you wish to receive updates). Providing contact information helps us to follow up if we need more information. However, we understand if you wish to remain anonymous. Note that anonymous reports may be more difficult to fully investigate.

* Urgency: If you believe the matter involves an imminent threat to life or serious illegal activity, clearly state this in your report and mark it as "URGENT."

### 5.3 Complaint Handling:

* Acknowledgment: Due to extremely limited resources, we cannot guarantee acknowledgment of all complaints. We will prioritize acknowledging complaints that appear to involve serious illegal content, particularly "priority offences" under the OSA, or content that poses an immediate threat to a user's safety.

* Initial Assessment: A volunteer moderator will review the complaint to determine: 

* Whether the reported content or behavior falls within the scope of this procedure and whether the complaint aligns with the recommended measures in Ofcom's Codes of Practice, especially regarding reporting and complaint procedures. (ICU D1 to ICU D14).

* The urgency and severity of the issue, prioritizing illegal content, content harmful to children, and content posing an immediate threat. We will use Ofcom's guidance to help us assess the severity of the reported content.

* Whether the complaint provides sufficient information for our limited investigation.

* Whether the complaint relates to a potential breach of our Terms of Service by another user or by us.

* Investigation (Extremely Limited): Our investigation will be strictly limited to reviewing the reported content, the user's recent activity on the forum, and any evidence provided. We do not have dedicated investigative resources, legal expertise, or advanced technical tools. We will primarily rely on the information provided in the complaint and our own limited observations. We may not be able to conduct in-depth investigations in all cases.

* Decision and Action (Limited Options): Based on the investigation, our volunteer moderators will decide on the appropriate action within their capacity. Possible actions include: 

* No Action:

* If the content/behavior does not violate UK law, our community guidelines, or pose a significant risk of harm based on our assessment.

* If we lack the capacity or expertise to make a definitive determination within a reasonable timeframe.

* If the complaint is deemed to be vexatious or without merit.

* Warning: Issuing a formal warning to the user responsible for the content or behavior.

* Content Removal: Removing or editing the reported content if it violates our guidelines or is deemed illegal under the OSA.

* Account Suspension: Temporarily or permanently suspending the user's account, particularly in cases of repeated or serious violations.

* Reporting to Authorities: Reporting content or activity to the appropriate authorities (e.g., police, NCA, CEOP, IWF) if we believe it to be illegal under the OSA, particularly in cases of CSAM, terrorist content, or content posing an imminent threat to life or safety. We will prioritize reporting the most serious illegal content as required by law and our ethical obligations, and we will follow Ofcom's guidance on reporting to authorities.

* Communication:

* We will only be able to inform you of the outcome of your complaint if: 

* You provided contact information.

* The outcome directly affects you (e.g., you were the target of harassment).

* We have the resources to do so.

* We will not disclose specific actions taken against other users due to privacy concerns, resource constraints, and potential legal implications.

* Appeals (Very Limited): If you disagree with the outcome of your complaint regarding content moderation, you can appeal by contacting [Method, e.g., the forum administrator via private message]. Appeals will only be considered in exceptional circumstances, and only if the appeal is made about content that we have taken a decision on (i.e., it is not an appeal made in relation to a complaint about our alleged failure to uphold our terms of service), and when volunteer resources permit, due to severe resource limitations. Appeals are not available for complaints regarding our alleged failure to uphold our terms of service. (ICU D7 to ICU D10).

### 5.4 Dedicated Reporting Channel for Trusted Flaggers (ICU D14): 

* We will, where practicable and within our limited resources, establish and maintain a dedicated reporting channel for trusted flaggers, as defined by Ofcom's Codes of Practice. This channel will be designed to handle reports of potentially illegal content efficiently and effectively. Where we implement this measure, the dedicated reporting channel will adhere to measures ICU D14.3 to ICU D14.8 as far as possible within our limited resources, and will ensure that reports from trusted flaggers are prioritized and processed promptly. We will make our existing reporting channel available to trusted flaggers if it is run in accordance with measures ICU D14.3 to ICU D14.8. We will regularly review the operation of this channel and make improvements where possible. (ICU D14)

### 5.5 Processing Complaints from other Users: 

* We will make available a notice facility for users to report potentially illegal content that they encounter on our service. This facility will be easy to access and use, and will allow users to report content anonymously, if they so choose. We will ensure that reports received through this notice facility are assessed and that, where we become aware of illegal content, action is taken expeditiously to remove or disable access to that content. Where a user has provided contact details, we will endeavour to, within our limited resources, notify the user that we have received their complaint and, where we decide to take action to remove or disable access to content, we will notify the user of this decision and the reasons for it as soon as reasonably practicable. (ICU D1 to ICU D6)


## 6. Content Moderation and Safety Measures

In addition to our complaint process, we are taking the following steps to address online safety, within our extremely limited capacity:

* We will strive to align our content moderation practices with the recommended measures in Ofcom's Codes of Practice, particularly regarding the functions of content moderation teams (ICU C1-C4), prioritising measures that we are able to implement within our limited resources. However, our capacity in this area is severely restricted.

* Community Guidelines: We have established clear community guidelines [Link to Guidelines] that prohibit illegal and harmful content, as defined by the OSA, Ofcom's guidance, and our own standards. These guidelines specifically address the types of content listed as "priority offences" and content harmful to children ("Primary Priority Content" and "Priority Content") in the Act, as well as content related to self-harm, eating disorders, suicide, and various forms of abuse and hate, as outlined by Ofcom.

* Volunteer Moderators: Our volunteer moderators monitor the forum for content that violates our guidelines or the law. Their responsibilities include reviewing reported content, assessing it against our community guidelines and the requirements of the OSA, taking appropriate action to remove or disable access to illegal content, escalating serious cases to the appropriate authorities where necessary (and within our limited resources), and, where possible, providing feedback to users who have reported content (within our limited resources). However, their availability and capacity are severely limited. We cannot guarantee 24/7 coverage, comprehensive monitoring of all content, or immediate responses to all issues.

* Automated Filtering (Very Basic): We may use very basic automated filters to detect and block certain keywords or phrases associated with the most serious types of illegal content, such as CSAM or terrorist content. We do not have the resources for sophisticated AI-based moderation, proactive scanning of all content, or advanced filtering technologies. Any automated filtering will be limited to simple keyword matching and will be subject to human review where possible.

* User Reporting: We rely heavily on user reports to identify potentially problematic content. We encourage users to report anything they believe may be illegal or harmful, and we have tried to make the reporting process as clear and accessible as possible, as outlined in Section 5.

* User Controls (Limited): We offer limited features that allow users to customize their experience and potentially reduce exposure to unwanted content. These may include: 

* Blocking: Users can block other users, preventing them from interacting with their content or sending them private messages.

* Muting: Users may be able to mute specific keywords or phrases (if this feature is available and can be implemented with minimal resources).

* Terms of Service: Our terms of service [Link to Terms] include provisions related to user conduct, content standards, and our rights to remove content and suspend accounts. They also include provisions relating to the protection of children, if applicable, and provisions relating to compliance with the OSA. The terms of service emphasize our limited resources and capacity for content moderation and are regularly reviewed, subject to volunteer availability, to ensure they align with the OSA and Ofcom's guidance.

## 7. Illegal Content Risk Assessment Process

We have conducted an initial illegal content risk assessment, as required by the OSA. This assessment followed Ofcom's guidance, including the four-step methodology outlined in their "Risk Assessment Guidance and Risk Profiles" document.

### 7.1 Step 1: Understanding the Kinds of Illegal Content to be Assessed

We identified the 17 kinds of priority illegal content that need to be separately assessed. These are:

1. Terrorism

2. Child Sexual Exploitation and Abuse (CSEA) 

* Grooming

* Child Sexual Abuse Material (CSAM) – imagery

* Child Sexual Abuse Material (CSAM) - URLs

3. Hate

4. Harassment, stalking, threats, and abuse

5. Controlling or coercive behaviour

6. Intimate image abuse

7. Extreme pornography

8. Sexual exploitation of adults

9. Human trafficking

10. Unlawful immigration

11. Fraud and financial offences

12. Proceeds of crime

13. Drugs and psychoactive substances

14. Firearms, knives, and other weapons

15. Encouraging or assisting suicide

16. Foreign interference

17. Animal cruelty

We also identified whether there is a risk of other illegal content taking place on our service, including relevant non-priority illegal content. We consulted Ofcom's Risk Profiles and identified the key risk factors relevant to our service for each of the 17 kinds of priority illegal content. These are detailed in section 7.5.

### 7.2 Step 2: Assessing the Risk of Harm

We assessed the likelihood and impact of each of the 17 kinds of priority illegal content, and of any other illegal content which we identified as being likely to occur on our service (including non-priority illegal content), using all relevant evidence. These are detailed in section 7.5.

We then assessed the different ways in which the service is used, including ways that are unintended. Further, we identified whether there are any specific characteristics or functionalities of the service's design or operation, not covered in Ofcom's Risk Profiles, which could increase the risk of harm.

We considered the effectiveness of any existing control measures which could impact the level of risk of harm to service users.

We consulted the four Risk Level Tables to assign a risk level for each of the 17 kinds of priority illegal content, and any other illegal content. This risk level reflects the risk as it exists on the service at the time of the assessment, having had regard to the efficacy of any existing control measures we have in place. These are detailed in section 7.5.

We concluded the assessment of all the risks relating to each kind of illegal content, and the design and operation of the service.

### 7.3 Step 3: Deciding on Measures, Implementing, and Recording

We consulted Ofcom's Codes of Practice, checked which measures are recommended for our service, and decided to implement applicable measures to reduce the risk of harm to individuals/users.

We identified any additional measures that may be appropriate for our service.

We implemented all relevant measures.

We recorded the outcomes of the risk assessment.

### 7.4 Step 4: Reporting, Reviewing, and Updating

We reported on the illegal content risk assessment and measures through appropriate Governance and Accountability channels.

We will monitor the effectiveness of safety measures at reducing the risk of harm to users.

We will monitor developing risks and the level of risk exposure after appropriate measures are implemented (also known as residual risk).

We will review and/or update the risk assessment when appropriate, including before making any significant change to any aspect of the service's design or operation. Our assessment was conducted in accordance with Ofcom's guidance.

### 7.5 Risk Assessment Findings

* **User Base:** The demographics and interests of our users (to the extent we can ascertain them), noting we have limited data collection capabilities.

* **Forum Features:** The functionalities of our forum (e.g., posting, private messaging, file sharing (if applicable), and the limitations of these features).

* **Risk Factors:** Based on Ofcom's U2U Risk Profile, we have identified the following risk factors that apply to [Forum Name] as it relates to each of the 17 kinds of illegal content:

* **(Example based on a hypothetical forum)**

* **Forum topic:** Photography

* **Risk Factors Present:**

* 1e. Discussion forums and chat rooms

* 3a. Services with user profiles

* 4a. Services with user connections

* 4b. Services where users can form user groups or send group messages

* 5b. Services with direct messaging

* 5d. Services with commenting on content

* 5e. Services with posting images or videos

* 5g. Services with re-posting or forwarding of content

* 7a. Services where users can search for user-generated content

* 7b. Services with hyperlinks

* 8. Services with content and/or network recommender systems

* **User base demographics:** We believe our user base is primarily adults interested in photography. We have limited data on specific demographics.

* **Business model (revenue model and growth strategy):** We do not run ads or have a paid subscription model. We rely on donations.

* **Commercial profile:** We are a small, non-profit forum.

* **Existing Measures:**

* Community Guidelines that prohibit illegal content.

* Volunteer moderators who review reported content.

* Basic keyword filtering for extremely offensive terms.

* **Assessment:**

#### *Table 7.5.1 Risk Assessment Summary*

<table>
  <tr>
   <td>

**Kind of illegal harm**

</td>
   <td>

**Risk Factors Present (See [U2U Risk Profile Table 9.1 for definitions](/risk-assessment-guidance-and-risk-profiles.pdf))**

</td>
   <td>

**Assessment of Likelihood (Considering Risk Factors, Evidence, and Existing Measures)**

</td>
   <td>

**Assessment of Impact (Considering User Base, Forum Features, and Potential Harm)**

</td>
   <td>

**Overall Risk Level (Using Ofcom's Risk Level Tables)**

</td>
  </tr>
  <tr>
   <td>

Terrorism

</td>
   <td>

1e, 3a, 4a, 4b, 5b, 5d, 5e, 5g, 7a, 7b, 8

</td>
   <td>

Low. While several risk factors are present, the forum's topic (photography) is not typically associated with terrorism. Our existing measures, though basic, provide some mitigation.

</td>
   <td>

Low. The nature of the forum makes it unlikely that terrorist content would reach a large audience or have a severe impact.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Child Sexual Exploitation and Abuse (CSEA)

</td>
   <td>

</td>
   <td>

</td>
   <td>

</td>
   <td>

</td>
  </tr>
  <tr>
   <td>

Grooming

</td>
   <td>

1e, 2, 3a, 3a (Fake user profiles), 4a, 4b, 5b, 5d, 5f, 8

</td>
   <td>

Medium. Children may be present. The ability for users to communicate one-to-one (direct messaging) with users under 18 combined with the presence of multiple risk factors increases the likelihood.

</td>
   <td>

Medium. Potential for severe harm to child users if grooming occurs.

</td>
   <td>

Medium

</td>
  </tr>
  <tr>
   <td>

CSAM (Image-based)

</td>
   <td>

1e, 2, 5e

</td>
   <td>

Low. While image sharing is possible, our existing moderation and the forum's topic make it less likely for image-based CSAM to be prevalent.

</td>
   <td>

Medium. If it were to occur, the impact of sharing image-based CSAM is severe.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

CSAM (URLs)

</td>
   <td>

1e, 2, 4b, 5b, 7b

</td>
   <td>

Low. URL sharing is possible, but our existing moderation and the forum's topic make it less likely for CSAM URLs to be prevalent.

</td>
   <td>

Medium. If it were to occur, the impact of sharing CSAM URLs is severe.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Encouraging or assisting suicide

</td>
   <td>

1e, 3b, 4b, 5d, 5e, 5g, 7b, 8

</td>
   <td>

Low. The forum's topic is not directly related to suicide, and our community guidelines discourage such content.

</td>
   <td>

Medium. The potential impact on vulnerable individuals is severe.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Hate

</td>
   <td>

1e, 3a, 3b, 4b, 5a, 5b, 5d, 5e, 8

</td>
   <td>

Medium. The potential for users to post hateful comments exists. However, our moderation efforts and the general focus of the community on photography may mitigate this risk to some extent.

</td>
   <td>

Medium. Hateful content can cause significant distress and contribute to a hostile environment.

</td>
   <td>

Medium

</td>
  </tr>
  <tr>
   <td>

Harassment, stalking threats and abuse

</td>
   <td>

1e, 3a, 3a (Fake user profiles), 3b, 4a, 5a, 5b, 5d, 5e, 5f, 5g

</td>
   <td>

Medium. The ability for users to interact directly and potentially anonymously increases the likelihood. However, our moderation and reporting tools offer some protection.

</td>
   <td>

Medium. These actions can cause significant emotional harm to victims.

</td>
   <td>

Medium

</td>
  </tr>
  <tr>
   <td>

Controlling or coercive behaviour

</td>
   <td>

1e, 3a (Fake user profiles), 4a, 4b, 5b, 5f

</td>
   <td>

Low. While some risk factors are present, the forum's topic and our existing moderation make this type of illegal content less likely.

</td>
   <td>

Medium. This behaviour can have a severe and lasting impact on victims.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Drugs and psychoactive substances

</td>
   <td>

1e, 3a, 3b, 4a, 4b, 5b, 5c, 5e, 6, 7a, 7b, 8

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with drug dealing.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Firearms, knives and other weapons

</td>
   <td>

1e, 1f, 1g, 3b, 5c, 6, 7a

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with the sale of weapons.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Human trafficking

</td>
   <td>

1e, 1f, 3a, 4b, 5b, 5c, 5e, 5f, 6

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with human trafficking.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Unlawful immigration

</td>
   <td>

1e, 3a, 4b, 5b, 5c, 5e

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with unlawful immigration.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Sexual exploitation of adults

</td>
   <td>

1e, 1f, 3a, 5c, 6

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with sexual exploitation.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Extreme pornography

</td>
   <td>

1e, 5e, 7a

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with extreme pornography, and our community guidelines prohibit such content.

</td>
   <td>

Medium. The nature of this content means that even limited exposure can be harmful.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Intimate image abuse

</td>
   <td>

1e, 1g, 4b, 5b, 5e, 5g

</td>
   <td>

Medium. The ability to share images and the presence of direct messaging increase the risk.

</td>
   <td>

Medium. This can cause significant emotional distress and reputational damage.

</td>
   <td>

Medium

</td>
  </tr>
  <tr>
   <td>

Proceeds of crime

</td>
   <td>

1e, 3a, 3a (Fake user profiles), 5b, 7a

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with this type of illegal content.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Fraud and financial services

</td>
   <td>

1e, 1f, 3a, 3a (Fake user profiles), 4a, 4b, 5b, 5c, 5d, 6, 7a, 7b

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with fraud, and our existing measures provide some mitigation.

</td>
   <td>

Medium. Fraud can have a significant financial impact on victims.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Foreign interference offence

</td>
   <td>

1e, 3a, 3a (Fake user profiles), 3b, 4a, 4b, 5c, 5e, 5g, 7b, 8

</td>
   <td>

Low. The forum's topic (photography) is not typically associated with foreign interference.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
  <tr>
   <td>

Animal cruelty

</td>
   <td>

1e, 4b, 5a, 5d, 5e

</td>
   <td>

Low. While some risk factors are present, the forum's topic (photography) is not typically associated with animal cruelty. Our existing measures, though basic, provide some mitigation.

</td>
   <td>

Low. The nature of the forum makes it unlikely that this type of content would reach a large audience.

</td>
   <td>

Low

</td>
  </tr>
</table>

[U2U Risk Profile Table 9.1 for definitions](/risk-assessment-guidance-and-risk-profiles.pdf)

| Kind of Illegal Harm                                    | Risk Factors Listed as Key Kind of Illegal Harm in the U2U Risk Profile                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1. Terrorism                                           | 1a. Social media services, 1b. Messaging services, 1c. Gaming services, 1e. Discussion forums and chat rooms, 1f. Marketplace and listing services, 1g. File-storage and file-sharing services, 3b. Anonymous user profiles or users without accounts, 4a. User connections, 4b. Group messaging, 5a. Livestreaming, 5b. Direct messaging, 5c. Encrypted messaging, 5d. commenting on content, 5e. Posting images or videos, 7a. User-generated content searching, 7b. Hyperlinks and 8. Content and network recommender systems.                                                 |
| 2. Child sexual exploitation and abuse (CSEA)           | See 2A, 2B, 2B(i) and 2B(ii) below                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| 2A. Grooming                                           | 1a. Social media services, 1b. Messaging services, 1c. Gaming services, 1e. Discussion forums and chat rooms, 2. Child users (under 18s), 3a. User profiles, 3a. Fake user profiles, 4a. User connections, 4b. User groups, 4b. Group messaging, 5a. Livestreaming, 5b. Direct messaging, 5c. encrypted messaging, 5d. commenting on content, 5f. Posting or sending location information and 8. Network recommender systems.                                                                                                                                                                          |
| 2B. CSAM (including both image-based CSAM and CSAM URLs) | 1a. Social media services, 1b. Messaging services, 1e. Discussion forums and chat rooms, 2. Child users (under 18s), 3b. Anonymous user profiles or users without accounts, 4b. User groups, 4b. Group messaging, 5b. Direct messaging and 5c. Encrypted messaging.                                                                                                                                                                                                                                                                                                                          |
| 2B(i). Image-based CSAM                               | 1d. Adult services, 1g. File-storage and file-sharing services, 5a. Livestreaming and 5e. Posting images or videos.                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| 2B(ii). CSAM URLs                                      | 7b. Hyperlinks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 3. Encouraging or assisting suicide (or attempted suicide) | 1a. Social media services, 1b. Messaging services, 1e. Discussion forums and chat rooms, 3b. Anonymous user profiles or users without accounts, 4b. User groups, 5a. Livestreaming, 5d. commenting on content, 5e. Posting images or videos, 5g. Re-posting or forwarding content, 7b. Hyperlinking and 8. Content recommender systems.                                                                                                                                                                                                                                              |
| 4. Hate                                                | 1a. Social media services, 1b. Messaging services, 1c. Gaming services, 3a. User profiles, 3b. Anonymous user profiles or users without accounts, 4b. User groups, 5a. Livestreaming, 5b. Direct messaging, 5d. commenting on content, 5e. Posting images or videos and 8. Content recommender systems.                                                                                                                                                                                                                                                                                |
| 5. Harassment, stalking threats and abuse              | 1a. Social media services, 1b. Messaging services, 1c. Gaming services, 3a. User profiles, 3a. Fake user profiles, 3b. Anonymous user profiles or users without accounts, 4a. User connections, 5a. Livestreaming, 5b. Direct messaging, 5d. Commenting on content, 5e. Posting images or videos, 5f. Posting or sending location information and 5g. Re-posting or forwarding content.                                                                                                                                                                                                     |
| 6. Controlling or coercive behaviour                   | 1a. Social media services, 1b. Messaging services, 3a. Fake user profiles, 4a. User connections, 4b. User groups, 5b. Direct messaging, 5e. Posting images or videos and 5f. Posting or sending location information.                                                                                                                                                                                                                                                                                                                                                                |
| 7. Drugs and psychoactive substances                    | 1a. Social media services, 1b. Messaging services, 1f. Marketplace and listing services, 3a. User profiles, 3b. Anonymous user profiles or users without accounts, 4a. User connections, 4b. User groups, 5b. Direct messaging, 5c. Encrypted messaging, 5e. Posting images or videos, 6. Posting goods or services for sale, 7a. User-generated content searching, 7b. Hyperlinks and 8. Network recommender systems.                                                                                                                                                                            |
| 8. Firearms, knives and other weapons                   | 1a. Social media services, 1b. Messaging services, 1f. Marketplace and listing services, 1g. File-storage and file-sharing services, 3b. Anonymous user profiles or users without accounts, 5c. Encrypted messaging, 6. Posting goods or services for sale and 7a. User-generated content searching.                                                                                                                                                                                                                                                                                          |
| 9. Human trafficking                                   | 1a. Social media services, 1b. Messaging services, 1f. Marketplace and listing services, 3a. User profiles, 4b. User groups, 5b. Direct messaging, 5c. Encrypted messaging, 5e. Posting images or videos, 5f. Posting or sending location information and 6. Posting goods or services for sale.                                                                                                                                                                                                                                                                                            |
| 10. Unlawful immigration                               | 1a. Social media services, 1b. Messaging services, 3a. User profiles, 4b. User groups, 5b. Direct messaging, 5c. Encrypted messaging and 5e. Posting images or videos.                                                                                                                                                                                                                                                                                                                                                                                                                     |
| 11. Sexual exploitation of adults                      | 1a. Social media services, 1b. Messaging services, 1f. Marketplace and listing services, 3a. User profiles, 5c. Encrypted messaging and 6. Posting goods or services for sale.                                                                                                                                                                                                                                                                                                                                                                                                               |
| 12. Extreme pornography                               | 1a. Social media services, 1d. Adult services, 5e. Posting images or videos and 7a. User-generated content searching.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| 13. Intimate image abuse                              | 1a. Social media services, 1d. Adult services, 1e. Discussion forums and chat rooms, 1g. File-storage and file-sharing services, 4b. User groups, 4b. Group messaging, 5b. Direct messaging, 5e. Posting images or videos and 5g. Re-posting or forwarding content.                                                                                                                                                                                                                                                                                                                             |
| 14. Proceeds of crime                                 | 1a. Social media services, 1b. Messaging services, 3a. User profiles, 3a. Fake user profiles, 5b. Direct messaging and 7a. User-generated content searching.                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 15. Fraud and financial services                      | 1a. Social media services, 1b. Messaging services, 1f. Marketplace and listing services, 3a. User profiles, 3a. Fake user profiles, 4a. User connections, 4b. User groups, 4b. Group messaging, 5b. Direct messaging, 5c. Encrypted messaging, 5d. Commenting on content, 6. Posting goods or services for sale, 7a. User-generated content searching and 7b. Hyperlinking.                                                                                                                                                                                                                |
| 16. Foreign interference offence                      | 1a. Social media services, 1b. Messaging services, 1e. Discussion forums and chat rooms, 3a. User profiles, 3a. Fake user profile, 3b. Anonymous user profiles or users without accounts, 4a. User connections, 4b. User groups, 5c. Encrypted messaging, 5e. Posting images or videos, 5g. Re-posting or forwarding content, 7b. Hyperlinking and 8. Content recommender systems.                                                                                                                                                                                                                |
| 17. Animal cruelty                                     | 1a. Social media services, 1b. Messaging services, 4b. Group messaging, 5a. Livestreaming, 5d. Commenting on content and 5e. Posting images or videos.                                                                                                                                                                                                                                                                                                                                                                                                                                        |

## 8. Children's Access and Risk Assessment (if applicable)

Our approach to protecting children involves three interconnected assessments ([details](https://www.ofcom.org.uk/online-safety/protecting-children/age-checks-to-protect-children-online/)):

1. Children's Access Assessment: Determining whether children are likely to access our forum
2. Children's Risk Assessment: If children are likely to access our forum, assessing potential risks
3. Age Assurance Assessment: Evaluating and implementing appropriate age verification measures

* **Our assessment has identified** [List key risks to children, if any]. We have prioritized these risks based on their severity, likelihood, and potential impact, using Ofcom's guidance on children's risk assessments.

* **Mitigation Measures (if applicable):** To protect children, we have implemented the following measures: [List measures, e.g., We do not implement age verification checks due to cost prohibitions, we rely on user reports and proactive scanning as detailed in section 6, specific community guidelines for child safety]. We have based these measures on our risk assessment and Ofcom's codes of practice related to the protection of children.

* **Disclaimer:** These measures are extremely limited by our resources and may not be fully effective in preventing children from accessing or encountering harmful content. We cannot guarantee the same level of protection as larger platforms with dedicated resources and technology for age assurance and content moderation. We will regularly review these measures, subject to resource availability, and update them as needed based on our assessments and Ofcom's guidance.

### 8.1 **Children's Access Assessment:** 

Based on our forum's topic [Forum Topic], the nature of our content, and using Ofcom's guidance on conducting children's access assessments, we have assessed that [children are likely/are not likely] to access our forum. This assessment involved considering factors such as the likely user base, the presentation of the service, and the types of functionalities offered by our forum. We have documented the reasons for this conclusion, and we will review this assessment at least annually, or more frequently if there are significant changes to our forum or Ofcom's guidance.

### 8.2 **Children's Risk Assessment (if applicable):** 

Because [children are likely to access our forum], we have conducted a children's risk assessment, as required by the OSA. This assessment considered the types of harm that children might experience on our forum and the potential impact on them, based on our understanding of the OSA, Ofcom's guidance, and the categories of "Primary Priority Content" and "Priority Content." We have also considered any design or operational features of our forum that could increase or decrease the risk of harm to children.

### 8.3 **Age Assurance Assessment (if applicable):** 

Based on our children's access assessment and the nature of our content, we have determined that [choose one]:

    * Our forum requires age assurance measures because [explain reasoning]
    * Our forum does not require age assurance measures because [explain reasoning]

### 8.3.1 Compliance Timeline:

Our forum will implement all required age assurance measures by [July 2025](https://www.ofcom.org.uk/online-safety/protecting-children/age-checks-to-protect-children-online/), following this timeline:

* Q1 2025: Selection and testing of age assurance methods
* Q2 2025: Technical implementation and staff training
* Q3 2025: Full deployment and compliance monitoring

### 8.3.2 Our Age Assurance Approach:

We have selected the following methods based on our risk assessment, technical capabilities, and resource constraints:

Primary Method: [Choose from approved methods, such as:]
* Digital identity verification through approved third-party providers
* Email-based age estimation
* Photo ID matching

Backup Method: [Secondary verification if primary fails]

These methods have been chosen because they:
* Meet Ofcom's criteria for being highly effective
* Balance user privacy with child protection
* Can be implemented within our resource constraints
* Provide appropriate fallback options

We explicitly do not rely on:
* Self-declaration of age
* Payment methods that don't verify age
* Basic checkbox confirmations

### 8.3.3 Technical Implementation:

Our age verification system operates as follows:

1. Initial Access Protocol
* New users encounter age verification before accessing any restricted content
* Existing users will be required to complete verification upon their next login
* No restricted content is visible during the verification process

2. Verification Process
* Users are clearly informed about the verification requirement
* Multiple verification options are provided where possible
* Clear instructions guide users through each step
* Privacy implications are explicitly stated

3. Data Handling
* Age verification data is stored securely
* Only minimum necessary information is retained
* Regular data review and deletion protocols are in place

4. Failed Verification Handling
* Users who fail verification receive clear explanations
* Appeal process is available for false negatives
* Alternative verification methods are offered where appropriate

### 8.3.4 Privacy and User Experience Considerations:

Our age verification implementation prioritizes:

* Data Minimization: Collecting only essential information
* User Privacy: Implementing appropriate security measures
* Transparency: Clear communication about data usage
* Accessibility: Ensuring verification methods work for all valid users
* User Choice: Offering multiple verification options where possible

### 8.3.5 Documentation and Review Process:

We maintain comprehensive records of:
* Age verification implementation decisions
* Technical specifications and configurations
* Staff training materials and completion records
* Regular effectiveness reviews
* Incident reports and resolution details

Quarterly reviews assess:
* Verification system effectiveness
* User feedback and complaints
* Technical performance metrics
* Privacy impact assessments
* Compliance with current regulations

## 9. Freedom of Expression, Privacy, and Encryption

We are committed to upholding users' rights to freedom of expression within the bounds of the law and our community guidelines. We will consider the importance of freedom of expression, as outlined in the OSA, when making content moderation decisions. However, our capacity to make nuanced judgments on content that is considered "democratically important" or "journalistic," as defined in the Act, is severely limited. We lack the legal expertise and resources to thoroughly assess such content in every instance.

We will also respect user privacy and handle personal data in accordance with data protection laws (e.g., GDPR). We are deeply concerned about the potential implications of the OSA for privacy, particularly the provisions that could weaken end-to-end encryption or require extensive monitoring of user activity. We believe that strong encryption is essential for protecting users' privacy and security. We do not currently use end-to-end encryption on this forum, and any content scanning will be limited to the very basic automated filtering described in Section 6. We will closely monitor developments in this area and any guidance provided by Ofcom.

## 10. Record Keeping

We will maintain records of complaints, investigations, actions taken, and risk assessments to the best of our ability, in compliance with data protection regulations and the OSA, and with due regard to Ofcom's Codes of Practice, particularly regarding the complaints and appeals processes. (ICU D1 to ICU D14).. However, our record-keeping capacity is limited by our volunteer nature, lack of dedicated systems, and reliance on basic tools. We will prioritize record-keeping for cases involving illegal content and content harmful to children, particularly for any complaints related to our enforcement of our terms of service. We will use Ofcom's guidance on record-keeping to inform our practices, but our implementation will necessarily be limited by our resources.

We maintain records of:

* All complaints received, including details and actions taken.
* Risk assessments conducted and their outcomes.
* Actions taken in response to illegal content reports.
* We maintain detailed records of our age verification system.
    * Implementation documentation
    * Technical specifications
    * Effectiveness metrics
    * User feedback and complaints
    * System failures and resolutions
    * Regular review outcomes
    * Staff training records

These records are kept securely and are accessible for review as required by the OSA and data protection regulations. Due to our volunteer-run nature, record-keeping is managed using basic tools, prioritizing cases involving illegal content and child safety.


## 11. Review and Updates

This policy will be reviewed and updated periodically, or as needed to reflect changes in the law, Ofcom's guidance, or our forum's operations. These reviews will be subject to the availability of volunteers and will be limited in scope. We will prioritize updates related to the most significant changes in the OSA's requirements or Ofcom's guidance. We will aim to review this policy at least annually.

## 12. Contact Us

For questions or concerns about this policy or our online safety practices, please contact us at:

* Email: [Your Contact Email]
* Private Message: [Link to Moderator List]

We strive to respond to all inquiries as promptly as possible within our volunteer capacity.

## 13. Limitations and Disclaimer

This document represents our good faith effort to comply with the UK Online Safety Act as a small, volunteer-run online forum. We have extremely limited resources (financial, technical, and personnel) and cannot guarantee the same level of online safety, responsiveness, or investigative capacity as larger platforms with dedicated staff, advanced technology, and legal expertise.

Where we implement alternative measures, we ensure they meet the Act's requirements, particularly regarding freedom of expression and privacy. These alternatives are documented and reviewed regularly.

We may need to adjust our approach based on our capacity, evolving legal requirements, and guidance from Ofcom. We will prioritize addressing the most serious risks and harms, particularly those involving illegal content as defined by the OSA, and especially content related to the "priority offences" listed in the Act.

This document should not be considered legal advice. We have not had the benefit of formal legal counsel in creating this policy due to resource constraints.

## 14. User Redress Mechanisms

### 14.1 Internal Redress for Content Moderation Decisions:

* If you believe your content was wrongly removed or your account wrongly suspended, you may request a review by contacting [Method, e.g., the forum administrator via private message].

* Due to our limited resources, we can only offer a basic review based on the information available to us.

* We aim to respond to redress requests within [Timeframe, e.g., 14 days], but this may not always be possible.

### 14.2 Complaints About Our Enforcement of Terms of Service:

* If you believe we have failed to uphold our own Terms of Service [Link to Terms] in a way that has significantly impacted you, you may submit a complaint to [Designated Complaints Email or Contact Method].

* Your complaint should clearly state: 

* The specific provision of the Terms of Service you believe we have violated.

* How this alleged violation has affected you.

* The outcome you are seeking.

* We will acknowledge your complaint within [Timeframe, e.g., 7 days] if contact information is provided and resources permit.

* We will investigate your complaint to the best of our ability, given our limited resources, and aim to provide a response within [Timeframe, e.g., 30 days], in line with the timeframes outlined in Ofcom's Codes of Practice where applicable (ICU D7 to ICU D10). However, due to our volunteer capacity, delays may occur.

* Limitations: Our capacity to investigate and resolve such complaints is severely limited. We may not be able to provide detailed explanations for our decisions or offer remedies beyond those available through our standard content moderation process.

### 14.3 External Redress:

* While we hope to resolve issues internally, we acknowledge that you may wish to seek external recourse.

* You may consider reporting your concerns to relevant authorities or organizations, such as: 

* Ofcom: As the regulator for online safety, Ofcom may investigate complaints about systemic non-compliance with the OSA by online services. However, Ofcom does not typically handle individual content moderation disputes.

* Internet Watch Foundation (IWF): For reporting illegal online content, particularly child sexual abuse material.

* National Crime Agency (NCA)/CEOP: For reporting online child sexual exploitation and abuse.

* Police: For reporting other illegal online activities.

* Please note: We cannot guarantee the outcome of any external complaints or investigations.

Key Considerations and Potential Adjustments:

* Evolving Guidance: This template is based on the current understanding of the OSA and Ofcom's published guidance. As Ofcom releases further codes of practice and guidance, this template will need to be updated.

* Resource Constraints: The template reflects the severe limitations faced by small, volunteer-run forums. It's crucial to be realistic about what can be achieved with limited resources.

* Legal Advice: Despite the disclaimer, seeking legal advice remains essential for any forum attempting to comply with the OSA.

* Community Feedback: Regularly solicit and consider feedback from your user community on your online safety policies and practices.

## 15. Definitions and Interpretation

* **Risk of a Kind of Illegal Harm:** The potential for users to encounter specific types of illegal content, assessed based on Ofcom's Risk Profiles.

* **Multi-Risk Service:** A service that is at medium or high risk of two or more kinds of illegal harm.

* **Active United Kingdom User:** Any UK user who has accessed the user-to-user part of the service within the last month.

* **Trusted Flagger:** An individual or organization recognized by Ofcom for reliably reporting illegal content.

* **Large Service:** A service which has more than 7 million monthly active United Kingdom users.

* **Age Assurance:** Processes used to estimate or verify a user's age, including both age verification and age estimation methods.

* **Highly Effective Age Assurance:** Methods that meet Ofcom's criteria of being technically accurate, robust, reliable, and fair in determining whether users are children.

* **Age Verification:** Methods that definitively verify a user's age through documentary or biometric evidence.

* **Age Estimation:** Methods that estimate a user's age through technical means or data analysis.

* **Part 3 Services:** User-to-user services that must implement age assurance by July 2025 if likely to be accessed by children.

* **Part 5 Services:** Services that publish pornographic content and must implement age assurance immediately.

## 16. Index of Recommended Measures

An index of recommented measures relevant for forums.

| Recommended Measure | Code | Description |
|---|---|---|
| Annual review of risk management activities | ICU A1 |  Applies to large services and includes a review of illegal content risks and monitoring developing risks |
| Individual accountable for illegal content safety duties and reporting/complaints duties | ICU A2 | Applies to all services and requires a designated individual responsible for these duties |
| Written statements of responsibilities | ICU A3 | Applies to large or multi-risk services and requires written statements of responsibilities for senior managers handling illegal content risks |
| Internal monitoring and assurance | ICU A4 | Applies to large multi-risk services and requires an internal function to ensure measures against illegal content are effective |
| Tracking evidence of new and increasing illegal harm | ICU A5 | Applies to large or multi-risk services and requires tracking and reporting new types and increases in illegal content |
| Code of conduct regarding protection of users from illegal harm | ICU A6 | Applies to large or multi-risk services and requires a code of conduct for staff on protecting users from illegal content |
| Compliance training | ICU A7 | Applies to large or multi-risk services and requires training staff on compliance with illegal content duties |
| Terms of service: substance (all services) | ICU G1 | Applies to all services and requires terms of service to include illegal content protections, proactive technology information, and complaints processes | 
| Terms of service: clarity and accessibility | ICU G3 | Applies to all services and requires terms of service to be easy to find, accessible, and written in plain language |
| User blocking and muting | ICU J1 | Applies to large services with profiles and communication functions, and requires offering blocking and muting options for users |

## [Date of Last Update]

This policy was last updated on [Insert Date].

